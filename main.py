# -*- coding: utf-8 -*-
"""ImageCaptioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16f-mREF-81B0LDp4RGOHE5dZlC8n_2wY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import keras
import re
import nltk
from nltk.corpus import stopwords
import string
import json
from time import time
import pickle
from keras.applications.vgg16 import VGG16
from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from PIL import Image 
from keras.models import Model, load_model
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Input, Dense, Dropout, LSTM, Embedding
from keras.layers.merge import add
import collections

# Read caption
def readTextFile(path):
  with open(path) as f:
    captions = f.read()
  return captions

captions = readTextFile("/Flickr_TextData/Flickr8k.token.txt") #In the downloaded dataset
captions = captions.split('\n')[:-1]
print(len(captions))

#Dictionary to map list of captions it has with it
description = {}
for i in captions:
  first, second = i.split("\t")
  img_name = first.split(".")[0]
  desc = second
  # if the image id is already present or not
  if description.get(img_name) is None:
    description[img_name] = []
  description[img_name].append(desc)

"""###Data Cleaning"""

def clean_text(sentence):
  sentence = sentence.lower()
  sentence = re.sub("^[a-z]+"," ",sentence)
  sentence = sentence.split()
  sentence = [s for s in sentence if len(s)>1]
  sentence = " ".join(sentence)
  return sentence

for img_id, caption_list in description.items():
  for i in range(len(caption_list)):
    caption_list[i] = clean_text(caption_list[i])

#write the data to a text file for future use
f = open("description.txt","w")
f.write(str(description))
f.close()

"""###Creating a Vocablary"""

vocab = set()
for key in description.keys():
  [vocab.update(sentence.split()) for sentence in description[key]]

print(len(vocab))

#total words in the data set
total_words = []
for key in description.keys():
  [total_words.append(i) for des in description[key] for i in des.split()]
print(len(total_words))

#filter words from the vocab according to certain threshold frequency of occurance of words
counter = collections.Counter(total_words)
freq_count = dict(counter)
#sorting dictionary according to frequency count
sorted_freq_cnt = sorted(freq_count.items(), reverse = True, key = lambda x:x[1])
# Filter
threshold = 10
sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>10]
total_words = [x[0] for x in sorted_freq_cnt]

"""###Preparing training and testing dataset"""

train_filedata = readTextFile("./Flickr_TextData/Flickr_8k.trainImages.txt")#Download the dataset and you will get this path
test_filedata = readTextFile("./Flickr_TextData/Flickr_8k.testImages.txt")

train = [row.split(".")[0] for row in train_filedata.split("\n")[:-1]]
test = [row.split(".")[0] for row in test_filedata.split("\n")[:-1]]

#Prepare descriptions for training data
# add start and end tokend <s>, <e> tp our training data
train_desc = {}
for img_id in train:
  train_desc[img_id] =[]
  for cap in description[img_id]:
    cap_to_append = "<s> " + cap + " <e>"
    train_desc[img_id].append(cap_to_append)

"""###Transfer Learning
-Images ---> Features<br>
-Text ---> Features

**Image Feature Extraction**
"""

model = ResNet50(weights="imagenet", input_shape=(224,224,3))
model.summary()

new_model = Model(model.input, model.layers[-2].output)
new_model.summary()

def preprocess_img(img):
  img = Image.open(img)
  img = img.resize((224,224))
  img = np.array(img)
  img = np.expand_dims(img,axis=0)
  #Normalization
  img = preprocess_input(img)
  return img

Img_path = "./Images" # Download the dataset and give the path of image folder here

def encode_img(img):
  img = preprocess_img(img)
  feature = new_model.predict(img)
  feature = feature.reshape((-1,))
  return feature

'''encoding_train = {}
###Code for formation of pickle file that have been used in further coding directly by reading those files
#img_id -> feature extracted from resnet50
start = time()
for i, img_id in enumerate(train):
  img_path = Img_path + "/" + img_id + ".jpg"
  encoding_train[img_id] = encode_img(img_path)
  if i%100==0:
    print("encoding in progress %d"%i)
end =time()
print("Total time taken ", end-start)'''

#Store Features to disk
'''with open("encoded_train_feature.pkl","wb") as f:
  pickle.dump(encoding_train,f)'''

'''encoding_test = {}
#img_id -> feature extracted from resnet50
start = time()
for i, img_id in enumerate(test):
  img_path = Img_path + "/" + img_id + ".jpg"
  encoding_test[img_id] = encode_img(img_path)
  if i%100==0:
    print("encoding in progress %d"%i)
end =time()
print("Total time taken ", end-start)
#Store Features to disk
with open("encoded_test_feature.pkl","wb") as f:
  pickle.dump(encoding_test,f)'''

"""###Data Preprocessing for Caption"""

word_to_index = {}
index_to_word = {}
for i,word in enumerate(total_words):
  word_to_index[word] = i+1
  index_to_word[i+1] = word

index_to_word[1832] = "<s>"
index_to_word[1833] = "<e>"
word_to_index["<s>"] = 1832
word_to_index["<e>"] = 1833

max_length = 0
for key in train_desc.keys():
  for cap in train_desc[key]:
    max_length = max(max_length, len(cap.split()))
print(max_length)

vocab_size = len(index_to_word) + 1
print(vocab_size)

with open('./encoded_test_feature.pkl', 'rb') as f:
    data = pickle.load(f)
encoding_test = data

with open('./encoded_train_feature.pkl', 'rb') as f:
    data = pickle.load(f)
print(type(data))
encoding_train = data

"""###Data Loader/Generator"""

def data_generator(train_desc, encoding_train, word_to_index, max_len, batch_size):
  x1,x2,y = [],[],[]
  n = 0
  while True:
    for key,cap_list in train_desc.items():
      n+=1
      photo = encoding_train[ key]
      for cap in cap_list:
        seq = [word_to_index[word] for word in cap.split() if word in word_to_index]
        for i in range(1,len(seq)):
          xi = seq[0:i]
          yi = seq[i]
          #0 denotes padding word
          xi = pad_sequences([xi], maxlen=max_len, value = 0, padding = 'post')[0]
          yi = to_categorical([yi], num_classes=vocab_size)[0]
          x1.append(photo)
          x2.append(xi)
          y.append(yi)
        if n == batch_size:
          yield [[np.array(x1), np.array(x2)], np.array(y)]
          n = 0
          x1, x2, y = [], [], []

"""###Word Embedding"""

f = open("/content/drive/My Drive/Image segmentation/glove.6B.50d.txt")

embedding_index = {}
for line in f:
  values = line.split()
  words = values[0]
  word_embedding = np.array(values[1:], dtype='float')
  embedding_index[words] = word_embedding
  break

f.close()

def get_embedding_matrix():
  emb_dim = 50
  matrix = np.zeros((vocab_size, emb_dim))
  for word, index in word_to_index.items():
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
      matrix[index] = embedding_vector
  return matrix

embedding_matrix = get_embedding_matrix()
print(embedding_matrix.shape)

"""####Model Architecture"""

input_img_features = Input(shape = (2048,))
inp_img = Dropout(0.3)(input_img_features)
inp_img2 = Dense(256, activation = 'relu')(inp_img)

#Captions as input
input_captions = Input(shape=(max_length,))
inp_cap = Embedding(input_dim=vocab_size, output_dim=50, mask_zero=True)(input_captions)
inp_cap2 = Dropout(0.3)(inp_cap)
inp_cap3 = LSTM(256)(inp_cap2)

decoder1 = add([inp_img2, inp_cap3])
decoder2 = Dense(256, activation = "relu")(decoder1)
output = Dense(vocab_size, activation="softmax")(decoder2)

#Combined Model
model = Model(inputs = [input_img_features,input_captions], outputs = output)
model.summary()

model.layers[2].set_weights([embedding_matrix])
model.layers[2].trainable = False

model.compile(loss='categorical_crossentropy', optimizer='adam')

"""###Training of Model"""

'''###Trainning Model Code
epochs = 10
batch_len = 3
steps = len(train_desc)//batch_len

for i in range(epochs):
  generator = data_generator(train_desc= train_desc, encoding_train= encoding_train, word_to_index=word_to_index, max_len=max_length, batch_size=batch_len)
  model.fit_generator(generator, epochs=1,steps_per_epoch=steps, verbose=1)
  model.save("model_" + str(i) + ".h5")'''

model = load_model("./model_29.h5")

###Prediction

def prediction(photo):
  inp_text = "<s>"
  for i in range(max_length):
    sequence = [word_to_index[w] for w in inp_text.split() if w in word_to_index]
    sequence = pad_sequences([sequence], maxlen=max_length, padding = 'post')
    ypred = model.predict([photo, sequence])
    ypred = ypred.argmax() #word with maximum probability
    word = index_to_word[ypred]
    inp_text += (' ' + word)
    if word == "<e>":
      break
  final_caption = inp_text[1:-1]
  final_caption = ' '.join(final_caption)
  return final_caption

#Pick some random images and see results
for i in range(15):
  no = np.random.randint(0,1000)
  all_img_name = list(encoding_test.keys())
  img_name = all_img_name[no]
  photo_2048 = encoding_test[img_name].reshape((1,2048))
  j = plt.imread(Img_path+img_name+".jpg")
  plt.imshow(j)
  plt.axis("off")
  plt.show()
  caption = prediction(photo_2048)
  print(caption)
