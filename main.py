# -*- coding: utf-8 -*-
"""ImageCaptioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16f-mREF-81B0LDp4RGOHE5dZlC8n_2wY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import keras
import re
import nltk
from nltk.corpus import stopwords
import string
import json
from time import time
import pickle
from keras.applications.vgg16 import VGG16
from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from PIL import Image 
from keras.models import Model, load_model
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Input, Dense, Dropout, LSTM, Embedding
from keras.layers.merge import add
import collections

# Read caption
def readTextFile(path):
  with open(path) as f:
    captions = f.read()
  return captions

captions = readTextFile("/content/drive/My Drive/Image segmentation/Flickr_TextData/Flickr8k.token.txt")
captions = captions.split('\n')[:-1]
print(len(captions))

#Dictionary to map list of captions it has with it
description = {}
for i in captions:
  first, second = i.split("\t")
  img_name = first.split(".")[0]
  desc = second
  # if the image id is already present or not
  if description.get(img_name) is None:
    description[img_name] = []
  description[img_name].append(desc)

"""###Data Cleaning"""

def clean_text(sentence):
  sentence = sentence.lower()
  sentence = re.sub("^[a-z]+"," ",sentence)
  sentence = sentence.split()
  sentence = [s for s in sentence if len(s)>1]
  sentence = " ".join(sentence)
  return sentence

for img_id, caption_list in description.items():
  for i in range(len(caption_list)):
    caption_list[i] = clean_text(caption_list[i])

#write the data to a text file for future use
f = open("description.txt","w")
f.write(str(description))
f.close()

"""###Creating a Vocablary"""

vocab = set()
for key in description.keys():
  [vocab.update(sentence.split()) for sentence in description[key]]

print(len(vocab))

#total words in the data set
total_words = []
for key in description.keys():
  [total_words.append(i) for des in description[key] for i in des.split()]
print(len(total_words))

#filter words from the vocab according to certain threshold frequency of occurance of words
counter = collections.Counter(total_words)
freq_count = dict(counter)
#sorting dictionary according to frequency count
sorted_freq_cnt = sorted(freq_count.items(), reverse = True, key = lambda x:x[1])
# Filter
threshold = 10
sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>10]
total_words = [x[0] for x in sorted_freq_cnt]

"""###Preparing training and testing dataset"""

train_filedata = readTextFile("/content/drive/My Drive/Image segmentation/Flickr_TextData/Flickr_8k.trainImages.txt")
test_filedata = readTextFile("/content/drive/My Drive/Image segmentation/Flickr_TextData/Flickr_8k.testImages.txt")

train = [row.split(".")[0] for row in train_filedata.split("\n")[:-1]]
test = [row.split(".")[0] for row in test_filedata.split("\n")[:-1]]

#Prepare descriptions for training data
# add start and end tokend <s>, <e> tp our training data
train_desc = {}
for img_id in train:
  train_desc[img_id] =[]
  for cap in description[img_id]:
    cap_to_append = "<s> " + cap + " <e>"
    train_desc[img_id].append(cap_to_append)

"""###Transfer Learning
-Images ---> Features<br>
-Text ---> Features

**Image Feature Extraction**
"""

model = ResNet50(weights="imagenet", input_shape=(224,224,3))
model.summary()

new_model = Model(model.input, model.layers[-2].output)
new_model.summary()

def preprocess_img(img):
  img = Image.open(img)
  img = img.resize((224,224))
  img = np.array(img)
  img = np.expand_dims(img,axis=0)
  #Normalization
  img = preprocess_input(img)
  return img

Img_path = "/content/drive/My Drive/Image segmentation/Images"

def encode_img(img):
  img = preprocess_img(img)
  feature = new_model.predict(img)
  feature = feature.reshape((-1,))
  return feature

encoding_train = {}
#img_id -> feature extracted from resnet50
start = time()
for i, img_id in enumerate(train):
  img_path = Img_path + "/" + img_id + ".jpg"
  encoding_train[img_id] = encode_img(img_path)
  if i%100==0:
    print("encoding in progress %d"%i)
end =time()
print("Total time taken ", end-start)

#Store Features to disk
with open("encoded_train_feature.pkl","wb") as f:
  pickle.dump(encoding_train,f)

encoding_test = {}
#img_id -> feature extracted from resnet50
start = time()
for i, img_id in enumerate(test):
  img_path = Img_path + "/" + img_id + ".jpg"
  encoding_test[img_id] = encode_img(img_path)
  if i%100==0:
    print("encoding in progress %d"%i)
end =time()
print("Total time taken ", end-start)
with open("encoded_test_feature.pkl","wb") as f:
  pickle.dump(encoding_test,f)
